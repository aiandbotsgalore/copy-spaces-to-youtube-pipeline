name: Batch Ingest Spaces

on:
  push:
    paths:
      - 'batch_queue.txt'
  workflow_dispatch:
    inputs:
      parallel_jobs:
        description: 'Number of parallel downloads (1-10)'
        required: false
        type: number
        default: 10

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  # Read queue and set up matrix
  prepare:
    runs-on: ubuntu-latest
    outputs:
      urls: ${{ steps.read-queue.outputs.urls }}
      count: ${{ steps.read-queue.outputs.count }}
      has_more: ${{ steps.read-queue.outputs.has_more }}
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Read Queue
        id: read-queue
        run: |
          PARALLEL=${{ inputs.parallel_jobs || 10 }}

          if [[ ! -f batch_queue.txt ]]; then
            echo "No batch_queue.txt found"
            echo "urls=[]" >> $GITHUB_OUTPUT
            echo "count=0" >> $GITHUB_OUTPUT
            echo "has_more=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Get first N non-empty lines
          URLS=$(grep -v '^[[:space:]]*$' batch_queue.txt | head -n $PARALLEL | jq -R -s -c 'split("\n") | map(select(length > 0))')
          COUNT=$(echo "$URLS" | jq 'length')

          # Check if there are more after these
          TOTAL=$(grep -c -v '^[[:space:]]*$' batch_queue.txt || echo "0")
          if [ "$TOTAL" -gt "$PARALLEL" ]; then
            HAS_MORE="true"
          else
            HAS_MORE="false"
          fi

          echo "urls=$URLS" >> $GITHUB_OUTPUT
          echo "count=$COUNT" >> $GITHUB_OUTPUT
          echo "has_more=$HAS_MORE" >> $GITHUB_OUTPUT

          echo "Found $COUNT URLs to process ($TOTAL total in queue)"

  # Process URLs in parallel
  ingest:
    needs: prepare
    if: needs.prepare.outputs.count != '0'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        url: ${{ fromJson(needs.prepare.outputs.urls) }}
      fail-fast: false
      max-parallel: 10
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Install ffmpeg
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      - name: Set up Python
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38
        with:
          python-version: '3.10'

      - name: Install yt-dlp
        run: pip install yt-dlp

      - name: Extract Space ID
        id: extract
        run: |
          URL="${{ matrix.url }}"
          SPACE_ID=$(echo "$URL" | grep -oE '[a-zA-Z0-9]+$')
          echo "space_id=$SPACE_ID" >> $GITHUB_OUTPUT
          echo "Processing: $SPACE_ID"

      - name: Download Space
        id: download
        env:
          MANUAL_URL: ${{ matrix.url }}
        run: |
          mkdir -p work
          DATE=$(date +%Y%m%d)
          SPACE_ID="${{ steps.extract.outputs.space_id }}"

          echo "Downloading ${{ matrix.url }}..."

          yt-dlp \
            --retries 3 \
            --fragment-retries 3 \
            --no-playlist \
            --restrict-filenames \
            --extract-audio \
            --audio-format mp3 \
            --audio-quality 0 \
            --embed-metadata \
            --write-info-json \
            --output "work/${DATE}_${SPACE_ID}_%(title)s.%(ext)s" \
            "${{ matrix.url }}"

          MP3_FILE=$(find work -name "*.mp3" | head -1)
          JSON_FILE=$(find work -name "*.info.json" | head -1)

          if [[ -z "$MP3_FILE" ]]; then
            echo "::error::No MP3 produced"
            exit 1
          fi

          # Extract metadata
          if [[ -f "$JSON_FILE" ]]; then
            TITLE=$(python3 -c "import json; print(json.load(open('$JSON_FILE')).get('title','Unknown'))" 2>/dev/null || echo "Unknown")
            UPLOADER=$(python3 -c "import json; print(json.load(open('$JSON_FILE')).get('uploader','Unknown'))" 2>/dev/null || echo "Unknown")
            UPLOADER_ID=$(python3 -c "import json; print(json.load(open('$JSON_FILE')).get('uploader_id',''))" 2>/dev/null || echo "")
          else
            TITLE="Space_$SPACE_ID"
            UPLOADER="Unknown"
            UPLOADER_ID=""
          fi

          # Clean title
          TITLE=$(echo "$TITLE" | tr '_' ' ')

          echo "mp3_file=$MP3_FILE" >> $GITHUB_OUTPUT
          echo "title=$TITLE" >> $GITHUB_OUTPUT
          echo "uploader=$UPLOADER" >> $GITHUB_OUTPUT
          echo "uploader_id=$UPLOADER_ID" >> $GITHUB_OUTPUT
          echo "release_tag=${DATE}_${SPACE_ID}" >> $GITHUB_OUTPUT

      - name: Get Duration
        id: duration
        run: |
          DURATION=$(ffprobe -v error -show_entries format=duration \
            -of default=noprint_wrappers=1:nokey=1 "${{ steps.download.outputs.mp3_file }}" \
            | awk '{printf "%02d:%02d:%02d", ($1/3600), ($1%3600/60), ($1%60)}')
          echo "duration=$DURATION" >> $GITHUB_OUTPUT

      - name: Create Release
        uses: softprops/action-gh-release@c95fe1489396fe8a9eb87c0abf8aa5b2ef267fda
        with:
          tag_name: ${{ steps.download.outputs.release_tag }}
          name: "${{ steps.download.outputs.title }}"
          files: ${{ steps.download.outputs.mp3_file }}
          body: |
            **Title:** ${{ steps.download.outputs.title }}
            **Host:** ${{ steps.download.outputs.uploader }} (@${{ steps.download.outputs.uploader_id }})
            **Duration:** ${{ steps.duration.outputs.duration }}
            **Source:** ${{ matrix.url }}

            ---
            METADATA::DURATION::${{ steps.duration.outputs.duration }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Update queue and trigger next batch
  cleanup:
    needs: [prepare, ingest]
    if: always() && needs.prepare.outputs.count != '0'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
        with:
          ref: ${{ github.ref }}
          fetch-depth: 0

      - name: Remove Processed URLs
        run: |
          PARALLEL=${{ inputs.parallel_jobs || 10 }}

          # Remove first N lines that were processed
          if [[ -f batch_queue.txt ]]; then
            tail -n +$((PARALLEL + 1)) batch_queue.txt > batch_queue.tmp || true
            mv batch_queue.tmp batch_queue.txt

            REMAINING=$(grep -c -v '^[[:space:]]*$' batch_queue.txt || echo "0")
            echo "Remaining URLs: $REMAINING"
          fi

      - name: Commit Updated Queue
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add batch_queue.txt
          git commit -m "chore: processed batch, ${{ needs.prepare.outputs.count }} spaces done" || echo "No changes"
          git push

  # Generate RSS after all downloads
  rss:
    needs: [ingest, cleanup]
    if: always() && needs.prepare.outputs.count != '0'
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Generate RSS Feed
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
        run: |
          cat <<'PYEOF' > gen_rss.py
          import os, json, re, urllib.request
          from datetime import datetime
          from xml.sax.saxutils import escape

          repo = os.environ['REPO']
          token = os.environ['GH_TOKEN']

          pages_url = f"https://{repo.split('/')[0]}.github.io/{repo.split('/')[1]}/"

          # Fetch all releases with pagination
          releases = []
          page = 1
          while True:
              req = urllib.request.Request(f"https://api.github.com/repos/{repo}/releases?per_page=100&page={page}")
              req.add_header('Authorization', f'token {token}')
              with urllib.request.urlopen(req) as r:
                  data = json.loads(r.read())
              if not data: break
              releases.extend(data)
              page += 1

          print(f"Found {len(releases)} releases")

          items = []
          for rel in releases:
              if rel.get('draft') or rel.get('prerelease'): continue
              dt = datetime.strptime(rel['published_at'], "%Y-%m-%dT%H:%M:%SZ")
              date = dt.strftime("%a, %d %b %Y %H:%M:%S GMT")
              body = rel.get('body', '')
              dur_match = re.search(r'METADATA::DURATION::(\d{2}:\d{2}:\d{2})', body)
              duration = dur_match.group(1) if dur_match else "00:00:00"

              for asset in rel.get('assets', []):
                  if asset['name'].endswith('.mp3'):
                      items.append(f'''<item>
            <title>{escape(rel['name'])}</title>
            <description>{escape(rel['name'])} - Twitter Space</description>
            <pubDate>{date}</pubDate>
            <enclosure url="{asset['browser_download_url']}" length="{asset['size']}" type="audio/mpeg"/>
            <guid isPermaLink="false">{asset['id']}</guid>
            <itunes:duration>{duration}</itunes:duration>
          </item>''')

          rss = f'''<?xml version="1.0" encoding="UTF-8"?>
          <rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">
          <channel>
            <title>My Spaces Archive</title>
            <link>{pages_url}</link>
            <description>Archived Twitter Spaces</description>
            <itunes:author>Logan Black</itunes:author>
            {''.join(items)}
          </channel>
          </rss>'''

          with open('podcast.xml', 'w') as f: f.write(rss)
          print(f"Generated RSS with {len(items)} episodes")
          PYEOF
          python3 gen_rss.py

      - name: Deploy to Pages
        run: |
          mkdir -p site
          cp podcast.xml site/
          [ -f artwork.jpg ] && cp artwork.jpg site/

      - name: Upload Pages Artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

      - name: Deploy Pages
        uses: actions/deploy-pages@v4
